% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rTensor_Decomp.R
\name{tucker.nonneg}
\alias{tucker.nonneg}
\title{sparse (semi-)nonnegative Tucker decomposition}
\usage{
tucker.nonneg(tnsr, ranks, core_nonneg = TRUE, tol = 1e-04, hosvd = FALSE,
  max_iter = 500, max_time = 0, lambda = rep.int(0, length(ranks) + 1),
  L_min = 1, rw = 0.9999, bound = Inf, U0 = NULL, Z0 = NULL,
  verbose = FALSE, unfold_tnsr = length(dim(tnsr)) * prod(dim(tnsr)) <
  4000^2)
}
\arguments{
\item{tnsr}{nonnegative tensor with \code{K} modes}

\item{ranks}{an integer vector of length \code{K} specifying the modes sizes for the output core tensor \code{Z}}

\item{core_nonneg}{constrain core tensor \code{Z} to be nonnegative}

\item{tol}{relative Frobenius norm error tolerance}

\item{hosvd}{If TRUE, apply High Order SVD to improve initial U and Z.}

\item{max_iter}{maximum number of iterations if error stays above \code{tol}}

\item{max_time}{max running time}

\item{lambda}{\code{K+1} vector of sparsity regularizer coefficients for the factor matrices and the core tensor}

\item{L_min}{lower bound for Lipschitz constant for the gradients of residual error \eqn{l(Z,U) = fnorm(tnsr - ttl(Z, U))} by \code{Z} and each \code{U}}

\item{rw}{controls the extrapolation weight}

\item{bound}{upper bound for the elements of \code{Z} and \code{U[[n]]} (the ones that have zero regularization coefficient \code{lambda})}

\item{U0}{initial factor matrices, defaults to nonnegative Gaussian random matrices}

\item{Z0}{initial core tensor \code{Z}, defaults to nonnegative Gaussian random tensor}

\item{verbose}{more output algorithm progress}

\item{unfold_tnsr}{precalculate \code{tnsr} to matrix unfolding by every mode (speeds up calculation, but may require lots of memory)}
}
\value{
a list:\describe{
\item{\code{U}}{nonnegative factor matrices}
\item{\code{Z}}{nonnegative core tensor}
\item{\code{est}}{estimate \eqn{Z \times_1 U_1 \ldots \times_K U_K}}
\item{\code{conv}}{method convergence indicator}
\item{\code{resid}}{the Frobenius norm of the residual error \code{l(Z,U)} plus regularization penalty (if any)}
\item{\code{n_iter}}{number of iterations}
\item{\code{n_redo}}{number of times Z and U were recalculated to avoid the increase in objective function}
\item{\code{diag}}{convergence info for each iteration\describe{
\item{\code{all_resids}}{residues}
\item{\code{all_rel_resid_deltas}}{residue delta relative to the current residue}
\item{\code{all_rel_resids}}{residue relative to the \code{sqrt(||tnsr||)}}
}}}
}
\description{
Decomposes nonnegative tensor \code{tnsr} into core optionally nonnegative tensor \code{Z} and sparse nonnegative factor matrices \code{U[n]}.
}
\details{
The function uses the alternating proximal gradient method to solve the following optimization problem:
\deqn{\min 0.5 \|tnsr - Z \times_1 U_1 \ldots \times_K U_K \|_{F^2} +
\sum_{n=1}^{K} \lambda_n \|U_n\|_1 + \lambda_{K+1} \|Z\|_1, \;\text{where}\; Z \geq 0, \, U_i \geq 0.}
If \code{core_nonneg} is \code{FALSE}, core tensor \code{Z} is allowed to have negative
elements and \eqn{z_{i,j}=max(0,z_{i,j}-\lambda_{K+1}/L_{K+1})} rule is replaced by \eqn{z_{i,j}=sign(z_{i,j})max(0,|z_{i,j}|-\lambda_{K+1}/L_{K+1})}.
The method stops if either the relative improvement of the error is below the tolerance \code{tol} for 3 consequitive iterations or
both the relative error improvement and relative error (wrt the \code{tnsr} norm) are below the tolerance.
Otherwise it stops if the maximal number of iterations or the time limit were reached.
}
\note{
The implementation is based on ntds() MATLAB code by Yangyang Xu and Wotao Yin.
}
\references{
Y. Xu, "Alternating proximal gradient method for sparse nonnegative Tucker decomposition", Math. Prog. Comp., 7, 39-70, 2013.
}
\seealso{
\code{\link{tucker}}

\url{http://www.caam.rice.edu/~optimization/bcu/}
}

